{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type xlm-roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer, TFXLMRobertaForMaskedLM, XLMRobertaConfig\n",
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "\n",
    "\n",
    "model_name = 'distill'\n",
    "config = XLMRobertaConfig.from_pretrained('intfloat/multilingual-e5-small')\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('intfloat/multilingual-e5-small')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFXLMRobertaForMaskedLM were initialized from the model checkpoint at kaggle/working/teacher_e5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfxlm_roberta_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFXLMRobertaMainL  multiple                  117505920 \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " lm_head (TFXLMRobertaLMHea  multiple                  96610997  \n",
      " d)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117904565 (449.77 MB)\n",
      "Trainable params: 21692213 (82.75 MB)\n",
      "Non-trainable params: 96212352 (367.02 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "teacher_model = TFXLMRobertaForMaskedLM.from_pretrained('kaggle/working/teacher_e5')\n",
    "teacher_model.roberta.embeddings.trainable = False\n",
    "teacher_model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINING_BATCH_SIZE = 126\n",
    "PRETRAINING_LEARNING_RATE = 5e-4\n",
    "PRETRAINING_EPOCHS = 8\n",
    "VOCABULARY_SIZE = 250002\n",
    "MASK_RATE = 0.25\n",
    "PREDICTIONS_PER_SEQ = 32\n",
    "SEQ_LENGTH = 512\n",
    "\n",
    "\n",
    "folder = 'dataset/processed_uncased_blanklines/'\n",
    "file_list = os.listdir(folder)\n",
    "file_list = [f\"{folder}/{_file}\" for _file in file_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file_list = [\n",
    "#  'dataset/processed_uncased_blanklines/talpco_indonesia.txt',\n",
    " 'dataset/processed_uncased_blanklines/kompas.txt',\n",
    " 'dataset/processed_uncased_blanklines/tempo.txt',\n",
    "#  'dataset/processed_uncased_blanklines/jw300.txt',\n",
    "#  'dataset/processed_uncased_blanklines/13k_words.txt',\n",
    "#  'dataset/processed_uncased_blanklines/parallel_corpus.txt',\n",
    "#  'dataset/processed_uncased_blanklines/frog_storytelling.txt',\n",
    "#  'dataset/processed_uncased_blanklines/bppt.txt'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset text (/Users/mdaniyalk/.cache/huggingface/datasets/text/default-bbae8d6820499bb4/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b595d8f0f243119ba83eea6ce0934b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"text\", data_files={\"train\": new_file_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4883814cb64ec387e8bce5718b94f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/848075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 848075\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:XLA (`jit_compile`) is not yet supported on Apple M1/M2 ARM processors. Falling back to `jit_compile=False`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=1e-3, weight_decay_rate=0.01)\n",
    "\n",
    "teacher_model.compile(optimizer=optimizer, jit_compile=True, metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name=\"input_ids\")\n",
    "embedding = teacher_model.roberta\n",
    "outputs, _ = embedding(inputs, return_dict=False)\n",
    "outputs = outputs[0]\n",
    "model = tf.keras.Model(inputs, outputs, name=\"embedding_xlmroberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model.predict(lm_datasets['train']['input_ids'][:75000]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model.predict(lm_datasets['train']['input_ids'][75000:150000]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = model.predict(lm_datasets['train']['input_ids'][150000:]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2_1 = model.predict(lm_datasets['train']['input_ids'][:75000]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2_1 = model.predict(lm_datasets['train']['input_ids'][75000:150000]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm_datasets['train']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2_1 = model.predict(lm_datasets['train']['input_ids'][150000:225000]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2_1 = model.predict(lm_datasets['train']['input_ids'][225000:]).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_out_v1_1-225k.npy', output2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indo NLU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prosa = 'indonlu/dataset/smsa_doc-sentiment-prosa'\n",
    "emotion_twitter = 'indonlu/dataset/emot_emotion-twitter'\n",
    "absa_airy = 'indonlu/dataset/hoasa_absa-airy'\n",
    "absa_prosa = 'indonlu/dataset/casa_absa-prosa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_preprocess_masked_label.tsv', 'test_preprocess.tsv', 'train_preprocess.tsv', 'vocab_uncased.txt', 'vocab.txt', 'valid_preprocess.tsv']\n",
      "['valid_preprocess.csv', 'vocab_uncased.txt', 'train_preprocess.csv', 'test_preprocess.csv', 'vocab.txt', 'test_preprocess_masked_label.csv']\n",
      "['valid_preprocess.csv', 'vocab_uncased.txt', 'train_preprocess.csv', 'test_preprocess.csv', 'vocab.txt', 'test_preprocess_masked_label.csv']\n",
      "['valid_preprocess.csv', 'vocab_uncased.txt', 'train_preprocess.csv', 'test_preprocess.csv', 'vocab.txt', 'test_preprocess_masked_label.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(sentiment_prosa))\n",
    "print(os.listdir(emotion_twitter))\n",
    "print(os.listdir(absa_airy))\n",
    "print(os.listdir(absa_prosa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "\n",
    "def group_texts(tokens_list):\n",
    "    total_length = len(tokens_list)\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    # result = [tokens_list[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    result = tokens_list[:block_size]\n",
    "    result += [1] * (block_size - len(result))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentiment_prosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentiment_prosa = pd.read_csv(f'{sentiment_prosa}/train_preprocess.tsv', sep='\\t', header=None)\n",
    "train_sentiment_prosa.columns = ['text', 'label']\n",
    "train_sentiment_prosa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_prosa_token = []\n",
    "for token in train_sentiment_prosa['text']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    train_sentiment_prosa_token.append(_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "train_sentiment_prosa_embedding = model.predict(train_sentiment_prosa_token[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 384)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentiment_prosa_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentiment_prosa_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('downstream/e5-small/smsa_doc-sentiment-prosa/train.npy', train_sentiment_prosa_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 19s 464ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_sentiment_prosa = pd.read_csv(f'{sentiment_prosa}/valid_preprocess.tsv', sep='\\t', header=None)\n",
    "valid_sentiment_prosa.columns = ['text', 'label']\n",
    "valid_sentiment_prosa.head()\n",
    "valid_sentiment_prosa_token = []\n",
    "for token in valid_sentiment_prosa['text']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    valid_sentiment_prosa_token.append(_token)\n",
    "valid_sentiment_prosa_embedding = model.predict(valid_sentiment_prosa_token)\n",
    "np.save('downstream/e5-small/smsa_doc-sentiment-prosa/valid.npy', valid_sentiment_prosa_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 8s 484ms/step\n"
     ]
    }
   ],
   "source": [
    "test_sentiment_prosa = pd.read_csv(f'{sentiment_prosa}/test_preprocess.tsv', sep='\\t', header=None)\n",
    "test_sentiment_prosa.columns = ['text', 'label']\n",
    "test_sentiment_prosa.head()\n",
    "test_sentiment_prosa_token = []\n",
    "for token in test_sentiment_prosa['text']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    test_sentiment_prosa_token.append(_token)\n",
    "test_sentiment_prosa_embedding = model.predict(test_sentiment_prosa_token)\n",
    "np.save('downstream/e5-small/smsa_doc-sentiment-prosa/test.npy', test_sentiment_prosa_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy</td>\n",
       "      <td>Ini adalah hal yang paling membahagiakan saat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anger</td>\n",
       "      <td>[USERNAME] [USERNAME] Dari pertama [USERNAME] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fear</td>\n",
       "      <td>yaudah kalo emang belum berani potong rambut p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fear</td>\n",
       "      <td>Jadi semalam, gw rekap, eh intinya yg gw usaha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anger</td>\n",
       "      <td>temen2 masa kecil yang turned out being asshol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0  happy  Ini adalah hal yang paling membahagiakan saat ...\n",
       "1  anger  [USERNAME] [USERNAME] Dari pertama [USERNAME] ...\n",
       "2   fear  yaudah kalo emang belum berani potong rambut p...\n",
       "3   fear  Jadi semalam, gw rekap, eh intinya yg gw usaha...\n",
       "4  anger  temen2 masa kecil yang turned out being asshol..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emotion_twitter = pd.read_csv(f'{emotion_twitter}/train_preprocess.csv')\n",
    "train_emotion_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 49s 444ms/step\n"
     ]
    }
   ],
   "source": [
    "train_emotion_twitter_token = []\n",
    "for token in train_emotion_twitter['tweet']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    train_emotion_twitter_token.append(_token)\n",
    "\n",
    "train_emotion_twitter_embedding = model.predict(train_emotion_twitter_token)\n",
    "\n",
    "np.save('downstream/e5-small/emot_emotion-twitter/train.npy', train_emotion_twitter_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 481ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_emotion_twitter = pd.read_csv(f'{emotion_twitter}/valid_preprocess.csv')\n",
    "\n",
    "valid_emotion_twitter_token = []\n",
    "for token in valid_emotion_twitter['tweet']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    valid_emotion_twitter_token.append(_token)\n",
    "\n",
    "valid_emotion_twitter_embedding = model.predict(valid_emotion_twitter_token)\n",
    "\n",
    "np.save('downstream/e5-small/emot_emotion-twitter/valid.npy', valid_emotion_twitter_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 497ms/step\n"
     ]
    }
   ],
   "source": [
    "test_emotion_twitter = pd.read_csv(f'{emotion_twitter}/test_preprocess.csv')\n",
    "\n",
    "test_emotion_twitter_token = []\n",
    "for token in test_emotion_twitter['tweet']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    test_emotion_twitter_token.append(_token)\n",
    "\n",
    "test_emotion_twitter_embedding = model.predict(test_emotion_twitter_token)\n",
    "\n",
    "np.save('downstream/e5-small/emot_emotion-twitter/test.npy', test_emotion_twitter_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## absa_airy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ac</th>\n",
       "      <th>air_panas</th>\n",
       "      <th>bau</th>\n",
       "      <th>general</th>\n",
       "      <th>kebersihan</th>\n",
       "      <th>linen</th>\n",
       "      <th>service</th>\n",
       "      <th>sunrise_meal</th>\n",
       "      <th>tv</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kebersihan kurang...</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sangat mengecewakan... hotel bad image, kebers...</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tempat nyaman bersih tapi tv terlalu tinggi ti...</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>pos</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semuanya bagus sesuai profile,dan harga promo ...</td>\n",
       "      <td>neut</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "      <td>pos</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tempat tidur sangat keras, bantal besar dan ke...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neg</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "      <td>neut</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review    ac air_panas   bau  \\\n",
       "0                               kebersihan kurang...  neut      neut  neut   \n",
       "1  sangat mengecewakan... hotel bad image, kebers...  neut      neut  neut   \n",
       "2  Tempat nyaman bersih tapi tv terlalu tinggi ti...  neut      neut  neut   \n",
       "3  semuanya bagus sesuai profile,dan harga promo ...  neut       neg  neut   \n",
       "4  Tempat tidur sangat keras, bantal besar dan ke...   neg       neg  neut   \n",
       "\n",
       "  general kebersihan linen service sunrise_meal    tv  wifi  \n",
       "0    neut        neg  neut    neut         neut  neut  neut  \n",
       "1    neut        neg  neut    neut         neut  neut  neut  \n",
       "2    neut        pos  neut    neut         neut   neg  neut  \n",
       "3     pos       neut  neut    neut         neut  neut  neut  \n",
       "4    neut       neut   neg    neut         neut  neut  neut  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_absa_airy = pd.read_csv(f'{absa_airy}/train_preprocess.csv')\n",
    "train_absa_airy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 33s 452ms/step\n"
     ]
    }
   ],
   "source": [
    "train_absa_airy_token = []\n",
    "for token in train_absa_airy['review']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    train_absa_airy_token.append(_token)\n",
    "\n",
    "train_absa_airy_embedding = model.predict(train_absa_airy_token)\n",
    "\n",
    "np.save('downstream/e5-small/hoasa_absa-airy/train.npy', train_absa_airy_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 4s 471ms/step\n",
      "9/9 [==============================] - 4s 475ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_absa_airy = pd.read_csv(f'{absa_airy}/valid_preprocess.csv')\n",
    "\n",
    "valid_absa_airy_token = []\n",
    "for token in valid_absa_airy['review']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    valid_absa_airy_token.append(_token)\n",
    "\n",
    "valid_absa_airy_embedding = model.predict(valid_absa_airy_token)\n",
    "\n",
    "np.save('downstream/e5-small/hoasa_absa-airy/valid.npy', valid_absa_airy_embedding)\n",
    "\n",
    "test_absa_airy = pd.read_csv(f'{absa_airy}/test_preprocess.csv')\n",
    "\n",
    "test_absa_airy_token = []\n",
    "for token in test_absa_airy['review']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    test_absa_airy_token.append(_token)\n",
    "\n",
    "test_absa_airy_embedding = model.predict(test_absa_airy_token)\n",
    "\n",
    "np.save('downstream/e5-small/hoasa_absa-airy/test.npy', test_absa_airy_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## absa_prosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>fuel</th>\n",
       "      <th>machine</th>\n",
       "      <th>others</th>\n",
       "      <th>part</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saya memakai Honda Jazz GK5 tahun 2014 ( perta...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avanza kenapa jadi boros bensin begini dah ah....</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saran ku dan pengalaman ku , mending beli mobi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dari segi harga juga pajero lebih mahal 30 jut...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kalo menurut gw enak pajero si</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      fuel   machine  \\\n",
       "0  Saya memakai Honda Jazz GK5 tahun 2014 ( perta...   neutral   neutral   \n",
       "1  Avanza kenapa jadi boros bensin begini dah ah....  negative   neutral   \n",
       "2  saran ku dan pengalaman ku , mending beli mobi...  positive  positive   \n",
       "3  Dari segi harga juga pajero lebih mahal 30 jut...   neutral   neutral   \n",
       "4                     Kalo menurut gw enak pajero si   neutral   neutral   \n",
       "\n",
       "     others     part     price  service  \n",
       "0  positive  neutral   neutral  neutral  \n",
       "1   neutral  neutral   neutral  neutral  \n",
       "2   neutral  neutral   neutral  neutral  \n",
       "3   neutral  neutral  positive  neutral  \n",
       "4  positive  neutral   neutral  neutral  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_absa_prosa = pd.read_csv(f'{absa_prosa}/train_preprocess.csv')\n",
    "train_absa_prosa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 12s 435ms/step\n"
     ]
    }
   ],
   "source": [
    "train_absa_prosa_token = []\n",
    "for token in train_absa_prosa['sentence']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    train_absa_prosa_token.append(_token)\n",
    "\n",
    "train_absa_prosa_embedding = model.predict(train_absa_prosa_token)\n",
    "\n",
    "np.save('downstream/e5-small/casa_absa-prosa/train.npy', train_absa_prosa_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 401ms/step\n"
     ]
    }
   ],
   "source": [
    "valid_absa_prosa = pd.read_csv(f'{absa_prosa}/valid_preprocess.csv')\n",
    "\n",
    "\n",
    "valid_absa_prosa_token = []\n",
    "for token in valid_absa_prosa['sentence']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    valid_absa_prosa_token.append(_token)\n",
    "\n",
    "valid_absa_prosa_embedding = model.predict(valid_absa_prosa_token)\n",
    "\n",
    "np.save('downstream/e5-small/casa_absa-prosa/valid.npy', valid_absa_prosa_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 408ms/step\n"
     ]
    }
   ],
   "source": [
    "test_absa_prosa = pd.read_csv(f'{absa_prosa}/test_preprocess.csv')\n",
    "\n",
    "\n",
    "test_absa_prosa_token = []\n",
    "for token in test_absa_prosa['sentence']:\n",
    "    _token = tokenizer(token)\n",
    "    _token = group_texts(_token['input_ids'])\n",
    "    test_absa_prosa_token.append(_token)\n",
    "\n",
    "test_absa_prosa_embedding = model.predict(test_absa_prosa_token)\n",
    "\n",
    "np.save('downstream/e5-small/casa_absa-prosa/test.npy', test_absa_prosa_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill e5 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.AdamW` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.AdamW`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.AdamW`.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'build'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdill\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdistill_indo_e5-ckpt-2.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> 4\u001b[0m     distill_model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[1;32m      5\u001b[0m distill_model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/dill/_dill.py:272\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, ignore, **kwds)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(file, ignore\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    267\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    Unpickle an object from a file.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[39mreturn\u001b[39;00m Unpickler(file, ignore\u001b[39m=\u001b[39;49mignore, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/dill/_dill.py:419\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     obj \u001b[39m=\u001b[39m StockUnpickler\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj)\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39mgetattr\u001b[39m(_main_module, \u001b[39m'\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    421\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ignore:\n\u001b[1;32m    422\u001b[0m             \u001b[39m# point obj class to main\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39mload_model(filepath, safe_mode\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     40\u001b[0m         f\u001b[39m.\u001b[39mwrite(serialized_model)\n\u001b[1;32m     41\u001b[0m     \u001b[39m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[39m# a custom_object_scope.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     model \u001b[39m=\u001b[39m saving_lib\u001b[39m.\u001b[39;49mload_model(filepath, safe_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:275\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    272\u001b[0m             asset_store\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    274\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:240\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 240\u001b[0m     model \u001b[39m=\u001b[39m deserialize_keras_object(\n\u001b[1;32m    241\u001b[0m         config_dict, custom_objects, safe_mode\u001b[39m=\u001b[39;49msafe_mode\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    244\u001b[0m all_filenames \u001b[39m=\u001b[39m zf\u001b[39m.\u001b[39mnamelist()\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m _VARS_FNAME \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:710\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m     compile_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompile_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    709\u001b[0m     \u001b[39mif\u001b[39;00m compile_config:\n\u001b[0;32m--> 710\u001b[0m         instance\u001b[39m.\u001b[39;49mcompile_from_config(compile_config)\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mshared_object_id\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[1;32m    713\u001b[0m     record_object_after_deserialization(\n\u001b[1;32m    714\u001b[0m         instance, config[\u001b[39m\"\u001b[39m\u001b[39mshared_object_id\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    715\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/engine/training.py:3582\u001b[0m, in \u001b[0;36mModel.compile_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompile(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n\u001b[1;32m   3580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m   3581\u001b[0m     \u001b[39m# Create optimizer variables.\u001b[39;00m\n\u001b[0;32m-> 3582\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_variables)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'build'"
     ]
    }
   ],
   "source": [
    "import dill as pickle\n",
    "\n",
    "with open('distill_indo_e5-ckpt-2.pkl', 'rb') as file:\n",
    "    distill_model = pickle.load(file)\n",
    "distill_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_absa_prosa_embedding = distill_model.predict(test_absa_prosa_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
